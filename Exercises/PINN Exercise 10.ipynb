{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 1.**<br>\n",
    "Consider the quantity\n",
    "$$\\mathrm{MSE} = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]$$\n",
    "where $\\hat{\\theta}$ is an estimator of $\\theta$, and $\\mathrm{MSE}$ denotes mean squared error.<br>\n",
    "Show that\n",
    "$$\\mathrm{MSE} = \\mathbb{E}[(\\hat{\\theta} -\\mathbb{E}[\\hat{\\theta}])^2] + (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol.]**<br>\n",
    "Let modify the $\\mathrm{MSE}$ formula<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{MSE} & = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]\\\\\n",
    "& = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] + \\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\\n",
    "& = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] + \\mathbb{E}[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] - 2\\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}] - \\theta)]\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "Since $\\mathbb{E}[\\hat{\\theta}]$ and $\\theta$ are constants,\n",
    "$$\\mathbb{E}[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] = (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2$$\n",
    "Then, $\\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])]$ is the deviation of $\\hat{\\theta}$ and it is 0, so the last term become 0;<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "2\\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}] - \\theta)] & = 2(\\mathbb{E}[\\hat{\\theta}] - \\theta)\\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])] \\\\\n",
    "& = 0\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "Therefore, the $\\mathrm{MSE}$ becomes,<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{MSE} & =  \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] + \\mathbb{E}[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\\n",
    "& = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] + (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2 \n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "So, that $\\mathrm{MSE}$ can be decomposed to $Var(\\hat{\\theta})$ and $Bias$, which is called 'Bias-Variance Decomposition'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 2.**<br>\n",
    "Let $X_1, X_2, \\dots, X_n$ be $i.i.d.$ random variables. Let $\\bar{X} = \\frac{1}{n}\\sum^n_{i = 1} X_i$ denotes the sample mean, and let $K^2 = \\frac{1}{n}\\sum^n_{i = 1} (X_i - \\bar{X})^2$. Show that $K^2$ is a biased estimator for $\\sigma^2$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol.]**<br>\n",
    "Since $X_i$ are sampled from $i.i.d$, the mean and variance can be written like:\n",
    "$$E[X_i] = \\mu,\\ Var(X_i) = \\sigma^2$$\n",
    "where $\\mu$ is the mean of $X_i$, $\\sigma^2$ is the variance of $X_i$.<br>\n",
    "Then we can calculate the $K^2$<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "K^2 & = \\frac{1}{n}\\sum^n_{i = 1} (X_i - \\bar{X})^2 \\\\\n",
    "& = \\frac{1}{n}\\sum^n_{i = 1} (X_i - \\mu + \\mu- \\bar{X})^2 \\\\\n",
    "& = \\frac{1}{n} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - 2\\sum^n_{i = 1}(X_i - \\mu)(\\bar{X} - \\mu) + \\sum^n_{i = 1} (\\bar{X} - \\mu)^2 \\right] \n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "For the middle term, since the $\\bar{X}$ and $\\mu$ are constants,\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\sum^n_{i = 1}(X_i - \\mu)(\\bar{X} - \\mu) & = (\\bar{X} - \\mu)\\sum^n_{i = 1}(X_i - \\mu) \\\\\n",
    "& = (\\bar{X} - \\mu) n (\\bar{X} - \\mu) \\\\\n",
    "& = n(\\bar{X} - \\mu)^2\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "So $K^2$ is simplified to\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "K^2 & = \\frac{1}{n} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - 2n(\\bar{X} - \\mu)^2+ \\sum^n_{i = 1} (\\bar{X} - \\mu)^2 \\right] \\\\\n",
    "& = \\frac{1}{n} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2 \\right] \\\\\n",
    "& = \\frac{1}{n} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - \\sum^n_{i = 1}(\\bar{X} - \\mu)^2 \\right]\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "Then, using that $Var(\\bar{X}) = \\frac{\\sigma^2}{n}$, we calculate the expectation value of $K^2$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "E[S^2] & = \\frac{1}{n} \\left( E \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 \\right] - E \\left[ \\sum^n_{i = 1}(\\bar{X} - \\mu)^2 \\right] \\right) \\\\\n",
    "& = \\frac{1}{n} (n\\sigma^2 - n \\cdot \\frac{\\sigma^2}{n}) \\\\\n",
    "& = \\frac{1}{n} (n-1)\\sigma^2 \\\\\n",
    "& = \\frac{n-1}{n}\\sigma^2\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "So the value $K^2$ is a biased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 3.**<br>\n",
    "Let $X_1, X_2, \\dots, X_n$ be $i.i.d.$ random variables. Let $\\bar{X} = \\frac{1}{n}\\sum^n_{i = 1} X_i$ denotes the sample mean, and let $S^2 = \\frac{1}{n-1}\\sum^n_{i = 1} (X_i - \\bar{X})^2$. Show that $K^2$ is a biased estimator for $\\sigma^2$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol.]**<br>\n",
    "Since $X_i$ are sampled from $i.i.d$, the mean and variance can be written like:\n",
    "$$E[X_i] = \\mu,\\ Var(X_i) = \\sigma^2$$\n",
    "where $\\mu$ is the mean of $X_i$, $\\sigma^2$ is the variance of $X_i$.<br>\n",
    "Then we can calculate the $K^2$<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "S^2 & = \\frac{1}{n-1}\\sum^n_{i = 1} (X_i - \\bar{X})^2 \\\\\n",
    "& = \\frac{1}{n-1}\\sum^n_{i = 1} (X_i - \\mu + \\mu- \\bar{X})^2 \\\\\n",
    "& = \\frac{1}{n-1} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - 2\\sum^n_{i = 1}(X_i - \\mu)(\\bar{X} - \\mu) + \\sum^n_{i = 1} (\\bar{X} - \\mu)^2 \\right] \n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "For the middle term, since the $\\bar{X}$ and $\\mu$ are constants,\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\sum^n_{i = 1}(X_i - \\mu)(\\bar{X} - \\mu) & = (\\bar{X} - \\mu)\\sum^n_{i = 1}(X_i - \\mu) \\\\\n",
    "& = (\\bar{X} - \\mu) n (\\bar{X} - \\mu) \\\\\n",
    "& = n(\\bar{X} - \\mu)^2\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "So $K^2$ is simplified to\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "S^2 & = \\frac{1}{n-1} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - 2n(\\bar{X} - \\mu)^2+ \\sum^n_{i = 1} (\\bar{X} - \\mu)^2 \\right] \\\\\n",
    "& = \\frac{1}{n-1} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - n(\\bar{X} - \\mu)^2 \\right] \\\\\n",
    "& = \\frac{1}{n-1} \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 - \\sum^n_{i = 1}(\\bar{X} - \\mu)^2 \\right]\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "Then, using that $Var(\\bar{X}) = \\frac{\\sigma^2}{n}$, estimating the expectation value of $S^2$.\n",
    "<br><br>\n",
    "$\n",
    "\\begin{aligned}\n",
    "E[S^2] & = \\frac{1}{n-1} \\left( E \\left[ \\sum^n_{i = 1}(X_i - \\mu)^2 \\right] - E \\left[ \\sum^n_{i = 1}(\\bar{X} - \\mu)^2 \\right] \\right) \\\\\n",
    "& = \\frac{1}{n-1} (n\\sigma^2 - n \\cdot \\frac{\\sigma^2}{n}) \\\\\n",
    "& =  \\frac{1}{n-1} (n-1 )\\sigma^2\n",
    "\\end{aligned}\n",
    "$\n",
    "<br><br>\n",
    "So the value $S^2$ is a unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 4.**<br>\n",
    "(a)Let $X_1, X_2, \\dots, X_n$ be $i.i.d$ random variables with $\\mathrm{p.d.f.} f(x)$ and $\\mathrm{c.d.f.} F(x)$. Define $Y = \\mathrm{max}(X_1, X_2, \\dots, X_n)$. What is the distribution of $Y$.<br>\n",
    "(b)Define $Z = \\mathrm{min}(X_1, X_2, \\dots, X_n)$. What is the distribution of $Z$?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol.(a)]**<br>\n",
    "Let solve the $\\mathrm{c.d.f} F_Y(y)$ of $Y$\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "F_Y(y) & = P(Y \\leq y) \\\\\n",
    "& = P(\\mathrm{max(X_1, X_2, \\dots, X_n) \\leq y})\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "It is same as all of $X_i$ are smaller than $y$\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "F_Y(y) & = P(\\mathrm{max(X_1, X_2, \\dots, X_n) \\leq y}) \\\\\n",
    "& = P(X_1 \\leq y, X_2 \\leq y, \\dots, X_n \\leq y)\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "Since each $X_i$ are independent\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "F_Y(y) & = P(X_1 \\leq y)P(X_2 \\leq y) \\cdots P(X_n \\leq y) \\\\\n",
    "& = [F(y)]^n\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "Therefore, the $\\mathrm{p.d.f.} f_Y(y)$ can be get by derivative of it.\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "f_Y(y) & = \\frac{d}{dy}F_Y(y) \\\\\n",
    "& = n[F(y)]^{n-1} f(y)\n",
    "\\end{aligned}$\n",
    "\n",
    "**[Sol.(b)]**<br>\n",
    "For the problem (b), it is also same as above<br>\n",
    "Solve the $\\mathrm{c.d.f} F_Z(z)$ of $Z$\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "F_Z(z) & = P(Z \\leq z) \\\\\n",
    "& = P(\\mathrm{max(X_1, X_2, \\dots, X_n) \\geq z})\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "It is same as all of $X_i$ are smaller than $z$, and we can use the complementary relation\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "F_Z(z) & = P(\\mathrm{max(X_1, X_2, \\dots, X_n) \\geq z}) \\\\\n",
    "& = P(X_1 \\geq z, X_2 \\geq z, \\dots, X_n \\geq z) \\\\\n",
    "& = P(X_1 \\geq z)P(X_2 \\geq z) \\cdots P(X_n \\geq z) \\\\\n",
    "& = [1 - P(X_1 \\leq z)][1 - P(X_2 \\leq z)] \\cdots [1 - P(X_n \\leq z)] \\\\\n",
    "& = [1 - F(z)]^n\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "Then, take derivative to the $\\mathrm{c.d.f}$ of $Z$\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "f_Z(z) & = \\frac{d}{dz}F_Z(z) \\\\\n",
    "& = n[1 - F(z)]^{n-1} f(z)\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 5.**<br>\n",
    "Consider a random sample of $x_1, x_2, \\dots , x_n$ from a uniform distribution $U(0, θ)$ with unknown parameter $θ$, where $θ > 0$. Determine the maximum likelihood estimator of $θ$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol.]**<br>\n",
    "As the random sample comes from the uniform distribution, they follow up the pdf\n",
    "$$\n",
    "f(x|\\theta) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{\\theta}, & x \\in [0, \\theta] \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "With using the pdf we can calculate the likelihood\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "L(\\theta) & = \\prod^n_{i = 0} f(x_i | \\theta) \\\\\n",
    "& = \\prod^n_{i = 0} \\frac{1}{\\theta} & (\\text{for} \\  0 < x_i < \\theta) \\\\\n",
    "& = \\frac{1}{\\theta^n}\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "As we take the $\\log$ to the likelihood function,\n",
    "<br><br>\n",
    "$\n",
    "\\log L(\\theta) = -n \\log \\theta \\quad \\text{for} \\  \\theta \\geq \\mathrm{max}(x_1, x_2, \\dots, x_n)\n",
    "$\n",
    "<br><br>\n",
    "To maximize likelihood,\n",
    "$$\\hat{\\theta}_{MLE} = \\max(x_1, x_2, \\dots, x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Problem 6.**<br>\n",
    "(a) Suppose random samples are given as $(0, 0, 1, 1, 0)$ from a binomial distribuition $b(1, \\theta)$ where $\\theta$ is unknown. Assume that $\\theta \\in (0, 1)$. What is the maximum likelihood estimator for $\\theta$?<br>\n",
    "(b) Suppose we impose the restriction that $\\theta \\in \\{ 0.2, 0.5, 0.7\\}$. What is the maximum likelihood estimator for $\\theta$?<br>\n",
    "(c) Assume that $\\theta \\in \\{ 0.2, 0.5, 0.7\\}$ and we have a prior distribution $\\pi_\\theta(0.2) = 0.1, \\pi_\\theta(0.5) = 0.01, \\pi_\\theta(0.7) = 0.89$. What is the maximum a posteriori estimator for $\\theta$?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sol. (a)]**<br>\n",
    "Depend on the random sample the likelihood is\n",
    "$$L(\\theta) = (1- \\theta)(1- \\theta)\\theta \\theta (1- \\theta) = \\theta^2 (1- \\theta)^3$$\n",
    "We can take $\\log$ to the likelihood function and then caclulate the derivative of it\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "\\log L(\\theta) & = 2\\log \\theta + 3\\log(1-\\theta) \\\\\n",
    "\\frac{d}{d\\theta} \\log L(\\theta) & = \\frac{2}{\\theta} - \\frac{3}{\\theta}\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "Find out the $\\theta$ value where the derivative becomes $0$.\n",
    "<br><br>\n",
    "$\\begin{aligned}\n",
    "\\frac{2}{\\theta} - \\frac{3}{\\theta} = 0 \\\\\n",
    "2 (1 - \\theta) = 3\\theta \\\\\n",
    "5 \\theta = 2 \\\\\n",
    "\\theta = \\frac{2}{5}\n",
    "\\end{aligned}$\n",
    "<br><br>\n",
    "So, the maximum likelihood estimator $\\hat{\\theta}_{(a)} = \\frac{2}{5}$\n",
    "\n",
    "**[Sol. (b)]**<br>\n",
    "From the set $\\{ 0.2, 0.5, 0.7\\}$,\n",
    "$$\\begin{aligned}\n",
    "L(0.2) = (0.2)^2(0.8)^3 = 0.00128 \\\\\n",
    "L(0.5) = (0.5)^2(0.5)^3 = 0.03125 \\\\\n",
    "L(0.7) = (0.7)^2(0.3)^3 = 0.01323 \\\\\n",
    "\\end{aligned}$$\n",
    "So, the maximum likelihood estimator $\\hat{\\theta}_{(b)} = 0.5$\n",
    "\n",
    "**[Sol. (c)]**<br>\n",
    "By maximum a posteriori estimating, we should use the prior probability\n",
    "$$\\pi_\\theta(0.2) = 0.1, \\pi_\\theta(0.5) = 0.01, \\pi_\\theta(0.7) = 0.89$$\n",
    "The posterior probability is proportional to:\n",
    "$$P(\\theta|X) \\propto L(\\theta)\\pi_\\theta(\\theta)$$\n",
    "Therefore, using the result from prob.(b)\n",
    "<br><br>\n",
    "$$\\begin{aligned}\n",
    "P(0.2 | X) \\propto 0.00128 \\cdot 0.1 & = 0.000128\\\\\n",
    "P(0.5 | X) \\propto 0.03125 \\cdot 0.01 & = 0.0003125\\\\\n",
    "P(0.7 | X) \\propto 0.01323 \\cdot 0.89 & = 0.01178\\\\\n",
    "\\end{aligned}$$\n",
    "So, the maximum a posterior estimator $\\hat{\\theta}_{(c)} = 0.7$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
